{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f5a82-30fb-475f-8886-a097bc2986d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def main():\n",
    "    # æ£€æŸ¥ PyTorch æ˜¯å¦èƒ½ç”¨ GPU\n",
    "    print(\"PyTorch CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "        print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "    # å®šä¹‰é‡‡æ ·å‚æ•°\n",
    "    sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=64)\n",
    "\n",
    "    # åˆå§‹åŒ– vLLM æ¨¡å‹ï¼ˆç”¨ä¸€ä¸ªå°æ¨¡å‹éªŒè¯ï¼Œé¿å…æ˜¾å­˜å‹åŠ›ï¼‰\n",
    "    print(\"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ facebook/opt-125m ...\")\n",
    "    llm = LLM(model=\"facebook/opt-125m\", tensor_parallel_size=1)\n",
    "\n",
    "    # è¿è¡Œä¸€æ¬¡æ¨ç†\n",
    "    outputs = llm.generate([\"Hello, can you confirm GPU is working?\"], sampling_params)\n",
    "\n",
    "    # æ‰“å°ç»“æœ\n",
    "    for output in outputs:\n",
    "        print(\"Prompt:\", output.prompt)\n",
    "        print(\"Generated:\", output.outputs[0].text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a04eb-7cb2-4af2-afa1-329307717ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_path = snapshot_download(\"facebook/opt-125m\")\n",
    "print(\"æ¨¡å‹ä¸‹è½½è·¯å¾„:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8904ea8-d7a9-4005-933a-94fadbbe4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def main():\n",
    "    print(\"PyTorch CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "        print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "    sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=64)\n",
    "\n",
    "    # ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹\n",
    "    model_path = \"llm_model/hugging/hf/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6\"\n",
    "    print(f\"ğŸš€ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}\")\n",
    "    llm = LLM(model=model_path, tensor_parallel_size=1,gpu_memory_utilization=0.8)\n",
    "\n",
    "    outputs = llm.generate([\"æ—©ä¸Šå¥½\"], sampling_params)\n",
    "\n",
    "    for output in outputs:\n",
    "        print(\"Prompt:\", output.prompt)\n",
    "        print(\"Generated:\", output.outputs[0].text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf8b4a-836e-4910-a8b6-4db4ac392b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"facebook/opt-125m\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, can you confirm GPU is working?\"}]\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedda0f6-8bf8-4b7f-941c-82b87b572743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘å¸®å¿™å†™ä¸€ä¸ªå‹æµ‹ä»£ç ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦æ˜ç¡®ç”¨æˆ·çš„éœ€æ±‚ã€‚å‹æµ‹é€šå¸¸æ˜¯æŒ‡å¯¹ç³»ç»Ÿè¿›è¡Œå‹åŠ›æµ‹è¯•ï¼Œæ¨¡æ‹Ÿå¤§é‡ç”¨æˆ·åŒæ—¶è®¿é—®ï¼Œä»¥æµ‹è¯•ç³»ç»Ÿçš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚ç”¨æˆ·å¯èƒ½å¸Œæœ›ç”¨Pythonæ¥å®ç°è¿™ä¸ªåŠŸèƒ½ï¼Œå¯èƒ½ç”¨äºæ€§èƒ½æµ‹è¯•ã€è´Ÿè½½æµ‹è¯•æˆ–è€…åˆ†å¸ƒå¼ç³»ç»Ÿçš„å‹åŠ›æµ‹è¯•ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„åœºæ™¯ã€‚ä»–ä»¬å¯èƒ½æ˜¯åœ¨å¼€å‘ä¸€ä¸ªWebåº”ç”¨ï¼Œæƒ³è¦æµ‹è¯•æœåŠ¡å™¨çš„å¤„ç†èƒ½åŠ›ï¼Œæˆ–è€…æ˜¯åœ¨æµ‹è¯•APIçš„å“åº”æ—¶é—´ã€‚ä¹Ÿæœ‰å¯èƒ½ä»–ä»¬éœ€è¦ä¸€ä¸ªç®€å•çš„è„šæœ¬æ¥æ¨¡æ‹Ÿå¹¶å‘è¯·æ±‚ï¼Œè§‚å¯Ÿç³»ç»Ÿçš„è¡¨ç°ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘éœ€è¦ç¡®å®šç”¨æˆ·çš„æŠ€æœ¯æ°´å¹³ã€‚å¦‚æœç”¨æˆ·æ˜¯åˆå­¦è€…ï¼Œå¯èƒ½éœ€è¦ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä½¿ç”¨Pythonçš„requestsåº“æˆ–è€…æ›´é«˜çº§çš„å·¥å…·å¦‚locustã€‚å¦‚æœæ˜¯é«˜çº§ç”¨æˆ·ï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„é…ç½®ï¼Œæ¯”å¦‚ä½¿ç”¨å¤šçº¿ç¨‹ã€å¼‚æ­¥ç¼–ç¨‹æˆ–è€…åˆ†å¸ƒå¼æµ‹è¯•ã€‚\n",
      "\n",
      "ç”¨æˆ·å¯èƒ½æ²¡æœ‰æ˜ç¡®è¯´æ˜çš„æ˜¯ä»–ä»¬éœ€è¦çš„å‹æµ‹ç±»å‹ï¼Œæ¯”å¦‚æ˜¯HTTPå‹æµ‹ã€æ•°æ®åº“å‹æµ‹ï¼Œè¿˜æ˜¯å…¶ä»–ç±»å‹çš„ç³»ç»Ÿæµ‹è¯•ã€‚ä¸è¿‡ï¼Œé€šå¸¸å‹æµ‹ä¼šæ¶‰åŠHTTPè¯·æ±‚ï¼Œæ‰€ä»¥å…ˆå‡è®¾æ˜¯HTTPå‹æµ‹ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘å¦‚ä½•å®ç°ã€‚Pythonä¸­å¸¸ç”¨çš„å‹æµ‹æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨requestsåº“å‘é€å¤§é‡è¯·æ±‚ï¼Œæˆ–è€…ä½¿ç”¨locustè¿›è¡Œæ›´å¤æ‚çš„æµ‹è¯•ã€‚ä¸è¿‡ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œæ‰€ä»¥å…ˆæä¾›ä¸€ä¸ªä½¿ç”¨requestsçš„ç¤ºä¾‹ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½éœ€è¦è€ƒè™‘å¹¶å‘æ§åˆ¶ï¼Œæ¯”å¦‚ä½¿ç”¨å¤šçº¿ç¨‹æˆ–å¼‚æ­¥æ¥æ¨¡æ‹Ÿå¤§é‡å¹¶å‘è¯·æ±‚ã€‚ä½†è¦æ³¨æ„ï¼Œè¿‡å¤šçš„å¹¶å‘å¯èƒ½å¯¼è‡´ç³»ç»Ÿè¿‡è½½ï¼Œæ‰€ä»¥éœ€è¦åˆç†è®¾ç½®å¹¶å‘æ•°ã€‚\n",
      "\n",
      "è¿˜éœ€è¦è€ƒè™‘æµ‹è¯•çš„æŒç»­æ—¶é—´ï¼Œç”¨æˆ·å¯èƒ½éœ€è¦ä¸€ä¸ªè„šæœ¬ï¼Œå¯ä»¥è¿è¡Œä¸€æ®µæ—¶é—´ï¼Œç„¶ååœæ­¢ï¼Œæˆ–è€…è‡ªåŠ¨è®°å½•ç»“æœã€‚å¯èƒ½éœ€è¦ä½¿ç”¨timeæ¨¡å—æ¥è®°å½•æ—¶é—´ï¼Œæˆ–è€…ä½¿ç”¨æ›´é«˜çº§çš„å·¥å…·å¦‚time.sleepæ¥æ§åˆ¶æµ‹è¯•æ—¶é—´ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½éœ€è¦ç»“æœåˆ†æï¼Œæ¯”å¦‚å“åº”æ—¶é—´ã€ååé‡ã€é”™è¯¯ç‡ç­‰ï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦è®°å½•è¿™äº›æ•°æ®å¹¶è¾“å‡ºåˆ°æ–‡ä»¶æˆ–å›¾è¡¨ä¸­ã€‚ä¸è¿‡ï¼Œå¦‚æœç”¨æˆ·åªéœ€è¦ä¸€ä¸ªç®€å•çš„è„šæœ¬ï¼Œå¯èƒ½ä¸éœ€è¦å¤æ‚çš„è¾“å‡ºå¤„ç†ã€‚\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘éœ€è¦ç¼–å†™ä¸€ä¸ªç¤ºä¾‹ä»£ç ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨requestsåº“å‘é€å¤§é‡GETè¯·æ±‚åˆ°ä¸€ä¸ªAPIç«¯ç‚¹ï¼Œæ¨¡æ‹Ÿå¹¶å‘ã€‚åŒæ—¶ï¼Œéœ€è¦è®¾ç½®åˆç†çš„å¹¶å‘æ•°ï¼Œæ¯”å¦‚100ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹å‘é€100ä¸ªè¯·æ±‚ï¼ŒæŒç»­5åˆ†é’Ÿã€‚\n",
      "\n",
      "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨requestsåº“æ—¶ï¼Œå¯èƒ½éœ€è¦å¤„ç†é‡è¯•ã€è¶…æ—¶ç­‰å¼‚å¸¸æƒ…å†µï¼Œä½†ç”¨æˆ·å¯èƒ½åªéœ€è¦åŸºç¡€ç¤ºä¾‹ï¼Œæ‰€ä»¥å¯ä»¥ç®€åŒ–å¤„ç†ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘ä½¿ç”¨å¤šçº¿ç¨‹æˆ–å¼‚æ­¥æ¥æé«˜æ•ˆç‡ï¼Œä½†éœ€è¦ç¡®ä¿ä»£ç çš„æ­£ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨threadingæ¨¡å—åˆ›å»ºå¤šä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹å‘é€è¯·æ±‚ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œå¦‚æœç”¨æˆ·å¸Œæœ›æ›´é«˜æ•ˆçš„å‹æµ‹ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨asyncioå’Œaiohttpï¼Œä½†è¿™ä¹Ÿå–å†³äºç”¨æˆ·çš„éœ€æ±‚ã€‚ä¸è¿‡ï¼Œå¯¹äºåˆå­¦è€…ï¼Œå¯èƒ½å…ˆæä¾›threadingçš„ç¤ºä¾‹ã€‚\n",
      "\n",
      "æœ€åï¼Œéœ€è¦æé†’ç”¨æˆ·æ³¨æ„æµ‹è¯•çš„è¾¹ç•Œæ¡ä»¶ï¼Œæ¯”å¦‚å¹¶å‘æ•°è¿‡é«˜å¯èƒ½å¯¼è‡´ç³»ç»Ÿå´©æºƒï¼Œå»ºè®®åœ¨æµ‹è¯•ç¯å¢ƒä¸­è¿›è¡Œï¼Œå¹¶ä¸”æ³¨æ„èµ„æºé™åˆ¶ï¼Œæ¯”å¦‚å†…å­˜ã€CPUç­‰ã€‚\n",
      "</think>\n",
      "\n",
      "ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨Pythonå®ç°çš„ç®€å•å‹æµ‹è„šæœ¬ç¤ºä¾‹ï¼Œç”¨äºæ¨¡æ‹Ÿå¤§é‡å¹¶å‘è¯·æ±‚å¹¶æµ‹è¯•ç³»ç»Ÿæ€§èƒ½ã€‚è¯¥è„šæœ¬ä½¿ç”¨`requests`åº“å‘é€HTTPè¯·æ±‚ï¼Œé€‚ç”¨äºWebæœåŠ¡æˆ–APIçš„æ€§èƒ½æµ‹è¯•ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… å‹æµ‹è„šæœ¬ï¼ˆåŸºç¡€ç‰ˆï¼‰ï¼šæ¨¡æ‹Ÿ100ä¸ªå¹¶å‘è¯·æ±‚\n",
      "```python\n",
      "import threading\n",
      "import time\n",
      "import requests\n",
      "from concurrent.futures import ThreadPoolExecutor\n",
      "\n",
      "# ç›®æ ‡URLï¼ˆæ›¿æ¢ä¸ºå®é™…æœåŠ¡åœ°å€ï¼‰\n",
      "TARGET_URL = \"https://api.example.com/endpoint\"\n",
      "\n",
      "# å¹¶å‘çº¿ç¨‹æ•°\n",
      "CONCURRENT_THREADS = 100\n",
      "# æ¯ä¸ªçº¿ç¨‹å‘é€çš„è¯·æ±‚æ¬¡æ•°\n",
      "REQUESTS_PER_THREAD = 100\n",
      "# æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰\n",
      "TEST_DURATION = 5\n",
      "\n",
      "def send_request(url):\n",
      "    try:\n",
      "        response = requests.get(url, timeout=10)\n",
      "        response.raise_for_status()\n",
      "        print(f\"Request successful: {response.status_code}\")\n",
      "    except requests.exceptions.RequestException as e:\n",
      "        print(f\"Request failed: {e}\")\n",
      "\n",
      "def main():\n",
      "    # åˆ›å»ºçº¿ç¨‹æ± \n",
      "    with ThreadPoolExecutor(max_workers=CONCURRENT_THREADS) as executor:\n",
      "        # æ¯ä¸ªçº¿ç¨‹å‘é€æŒ‡å®šæ•°é‡çš„è¯·æ±‚\n",
      "        for _ in range(CONCURRENT_THREADS):\n",
      "            executor.submit(send_request, TARGET_URL)\n",
      "\n",
      "    # ç­‰å¾…æµ‹è¯•å®Œæˆ\n",
      "    time.sleep(TEST_DURATION)\n",
      "    print(f\"Test completed after {TEST_DURATION} seconds.\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ“Œ ä½¿ç”¨è¯´æ˜\n",
      "1. **å®‰è£…ä¾èµ–**ï¼š\n",
      "   ```bash\n",
      "   pip install requests\n",
      "   ```\n",
      "\n",
      "2. **è¿è¡Œè„šæœ¬**ï¼š\n",
      "   ```bash\n",
      "   python pressure_test.py\n",
      "   ```\n",
      "\n",
      "3. **è¾“å‡ºç¤ºä¾‹**ï¼š\n",
      "   ```\n",
      "   Request successful: 200\n",
      "   Request successful: 200\n",
      "   ...\n",
      "   (100ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªå‘é€100æ¬¡è¯·æ±‚)\n",
      "   Test completed after 5 seconds.\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ§  æ‰©å±•å»ºè®®\n",
      "1. **ä½¿ç”¨Locustï¼ˆæ›´é«˜çº§ï¼‰**ï¼š\n",
      "   ```bash\n",
      "   pip install locust\n",
      "   locust -f locustfile.py\n",
      "   ```\n",
      "   é€šè¿‡å›¾å½¢ç•Œé¢ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚å“åº”æ—¶é—´ã€ååé‡ï¼‰ã€‚\n",
      "\n",
      "2. **åˆ†å¸ƒå¼å‹æµ‹**ï¼š\n",
      "   - ä½¿ç”¨`locust`çš„åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆ`locust -f locustfile.py --host=locust-host`ï¼‰ã€‚\n",
      "   - æˆ–ä½¿ç”¨`gunicorn`å’Œ`nginx`å®ç°è´Ÿè½½å‡è¡¡ã€‚\n",
      "\n",
      "3. **æ€§èƒ½åˆ†æ**ï¼š\n",
      "   - è®°å½•å“åº”æ—¶é—´ï¼š`start_time = time.time()`ï¼Œ`end_time = time.time()`ï¼Œ`duration = end_time - start_time`ã€‚\n",
      "   - è®°å½•é”™è¯¯ç‡ï¼š`try-except`å—æ•è·å¼‚å¸¸ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸš¨ æ³¨æ„äº‹é¡¹\n",
      "- **é¿å…è¿‡åº¦å‹æµ‹**ï¼šæµ‹è¯•æ—¶éœ€åœ¨éš”ç¦»ç¯å¢ƒä¸­è¿›è¡Œï¼Œé¿å…å½±å“çœŸå®ä¸šåŠ¡ã€‚\n",
      "- **èµ„æºé™åˆ¶**ï¼šé«˜å¹¶å‘å¯èƒ½æ¶ˆè€—å¤§é‡CPUå’Œå†…å­˜ï¼Œéœ€ç›‘æ§ç³»ç»Ÿèµ„æºã€‚\n",
      "- **å¼‚å¸¸å¤„ç†**ï¼šæ·»åŠ é‡è¯•æœºåˆ¶æˆ–æ—¥å¿—è®°å½•ä»¥æé«˜ç¨³å®šæ€§ã€‚\n",
      "\n",
      "å¦‚éœ€æ›´å¤æ‚çš„å‹æµ‹ï¼ˆå¦‚æ•°æ®åº“ã€æ¶ˆæ¯é˜Ÿåˆ—ï¼‰ï¼Œå¯ç»“åˆ`aiohttp`ã€`pytest`æˆ–`JMeter`ç­‰å·¥å…·å®ç°ã€‚éœ€è¦æˆ‘å¸®ä½ ç”Ÿæˆæ›´é«˜çº§çš„å‹æµ‹æ–¹æ¡ˆå—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 1. æŒ‡å®š vLLM æœåŠ¡å™¨åœ°å€\n",
    "# é»˜è®¤ vLLM server ä¼šåœ¨ http://localhost:8000 æä¾› OpenAI API å…¼å®¹æ¥å£\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")  \n",
    "# æ³¨æ„ï¼šapi_key å¯ä»¥éšä¾¿å¡«ï¼Œæ¯”å¦‚ \"EMPTY\"ï¼Œå› ä¸º vLLM é»˜è®¤ä¸åšé‰´æƒ\n",
    "\n",
    "# 2. è°ƒç”¨æ¨¡å‹ç”Ÿæˆ\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen3-1.7B\",  # ä½ åœ¨å¯åŠ¨ vLLM æ—¶åŠ è½½çš„æ¨¡å‹åç§°\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªpythonçš„åŠ©æ‰‹\"},\n",
    "        {\"role\": \"user\", \"content\": \"å¸®æˆ‘å†™ä¸€ä¸ªå‹æµ‹ä»£ç \"}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 3. æ‰“å°ç»“æœ\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f307811-e2f8-4553-aa2f-8f2a9940f36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
